This respository contains the code for the paper 'On the weak link between importance and prunability of attention heads' accepted at EMNLP 2020.

This work spans across two Deep Learning architectures for NLP, namely Transformers and BERT.

The datasets used are the following:<br>
(1) For Transformer: WMTâ€™14 English-Russian (EN-RU) and English-German (EN-DE) <br>
(2) For BERT: GLUE datasets(MNLI-m, QQP, QNLI and SST-2)<br>

The code was adapted from the following repositories: <br>
(1) Transformer: <a href="https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/models">https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/models
</a> <br>
(2) BERT: <a href="https://github.com/google-research/bert">https://github.com/google-research/bert</a>
